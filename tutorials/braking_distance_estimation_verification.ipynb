{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN surrogate for Braking Distance Estimation\n",
    "## Model Training, Proof of Monotonicity & Stability Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial illustrates the steps of training a surrogate model, defining input ranges, verifying monotonicity, computing upper bounds and checking stability on a case study of braking distance estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In the realm of modern transportation, functions that predict accurate estimation of braking distance are crucial to ensure safety as they may play a pivotal role in e.g. preventing accidents. As neural networks are increasingly becoming a prevalent solution, it is essential to verify their performance and robustness in order to maintain high safety standards.\n",
    "\n",
    "This tutorial presents a comprehensive approach to verifying braking distance estimation predicted by neural networks by mean of a verification pipeline that integrates multiple verification techniques aimed to ensure NN accuracy and reliability.\n",
    "\n",
    "<div>\n",
    "<img src=\"braking_distance_estimation_images/safe_surrogate.png\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "## Why Verification Matters\n",
    "\n",
    "Verification is the process of ensuring via guarantees that a system will behave as expected under various conditions. In the context of neural networks used for braking distance estimation, verification is vital because of:\n",
    "\n",
    "- Reliability: Verification ensures that the neural network performs consistently across different scenarios.\n",
    "\n",
    "- Compliance: Regulatory standards require rigorous verification of all components involved in safety-critical systems.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "This notebook assumes that the user has:\n",
    "\n",
    "- a basic understanding of neural networks and machine learning concepts\n",
    "\n",
    "- some familiarity with Python programming\n",
    "\n",
    "- an access to the required software tools and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Braking Distance Estimation\n",
    "\n",
    "The [Cessna 172 Skyhawk](https://en.wikipedia.org/wiki/Cessna_172) is a renowned American aircraft, known for its four-seat capacity, single-engine design, and high-wing, fixed-wing configuration. Manufactured by the Cessna Aircraft Company, the Skyhawk holds the title of the most successful aircraft in history, thanks to its remarkable longevity and widespread popularity. Since the delivery of the first production model in 1956, over 44,000 units had been built by 2015, a testament to its enduring appeal and reliability. The Cessna 172 continues to be produced today, maintaining its status as a staple in aviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "- If you run this notebook on Colab, install the following dependencies.\n",
    "- If you run this notebook locally, install dependencies using the provided pyproject.toml / requirements.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Colab: install the library\n",
    "on_colab = \"google.colab\" in str(get_ipython())\n",
    "if on_colab:\n",
    "    import sys  # noqa: avoid having this import removed by pycln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data & Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the safety of a simple braking distance estimation model where predictions are solely derived from information on pressure altitude and temperature.\n",
    "\n",
    "Download: The simulation code is available here as [cesna_simulation.py](https://github.com/ducoffeM/safety_braking_distance_estimation/blob/main/cesna_simulation.py) and should be stored in a subdirectory safety_braking_distance_estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/ducoffeM/safety_braking_distance_estimation.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "###\n",
    "import ipywidgets as widgets\n",
    "import keras\n",
    "import matplotlib\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import interact\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.models import Sequential\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import safety_braking_distance_estimation\n",
    "from safety_braking_distance_estimation.cesna_simulation import cesna_landing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airobas.blocks_hub.adv_block import CleverhansAdvBlock\n",
    "from airobas.blocks_hub.decomon_block import DecomonBlock\n",
    "from airobas.blocks_hub.gml_mip_block import GMLBrick\n",
    "from airobas.verif_pipeline import (\n",
    "    BoundsDomainBoxParameter,\n",
    "    BoundsDomainBoxParameterPerValueInterval,\n",
    "    BoundsDomainParameter,\n",
    "    ProblemContainer,\n",
    "    StabilityProperty,\n",
    "    StatusVerif,\n",
    "    compute_bounds,\n",
    "    full_verification_pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "all_logs = False\n",
    "if all_logs:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "else:\n",
    "    logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Train a model to predict the braking distance estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Ranges\n",
    "\n",
    "We here assume the following ranges for the model inputs:\n",
    "\n",
    "Temperature: 0Â°C to 40Â°C \\\n",
    "Pressure altitude: 0 feet to 4000 feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_temp = 0  # celsius\n",
    "MAX_temp = 40\n",
    "\n",
    "MIN_alt = 0\n",
    "MAX_alt = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(N, MIN=[MIN_temp, MIN_alt], MAX=[MAX_temp, MAX_alt]):\n",
    "    alpha_temp = np.array([np.random.rand() for _ in range(N)])\n",
    "    alpha_alt = np.array([np.random.rand() for _ in range(N)])\n",
    "\n",
    "    X = np.zeros((N, 2))\n",
    "    X[:, 0] = alpha_temp * MIN[0] + (1 - alpha_temp) * MAX[0]\n",
    "    X[:, 1] = alpha_alt * MIN[1] + (1 - alpha_alt) * MAX[1]\n",
    "\n",
    "    Y = [cesna_landing(X[i, 0], X[i, 1]) for i in range(N)]\n",
    "\n",
    "    return X, np.array(Y)  # samples, associated landing distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training, validation and test dataset\n",
    "\n",
    "\n",
    "X_train, y_train = generate_dataset(10000)\n",
    "X_valid, y_valid = generate_dataset(1000)\n",
    "X_test, y_test = generate_dataset(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize using mean and variance of the train dataset\n",
    "\n",
    "MEAN_x = np.mean(X_train, 0)[None]\n",
    "STD_x = np.std(X_train, 0)[None]\n",
    "\n",
    "print(MEAN_x)\n",
    "print(STD_x)\n",
    "\n",
    "X_train_ = (X_train - MEAN_x) / STD_x\n",
    "X_valid_ = (X_valid - MEAN_x) / STD_x\n",
    "X_test_ = (X_test - MEAN_x) / STD_x\n",
    "\n",
    "MEAN_y = np.mean(y_train, 0)\n",
    "STD_y = np.std(y_train, 0)\n",
    "\n",
    "y_train_ = (y_train - MEAN_y) / STD_y\n",
    "y_valid_ = (y_valid - MEAN_y) / STD_y\n",
    "y_test_ = (y_test - MEAN_y) / STD_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition, training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Dense(20, input_dim=2, activation=\"relu\"), Dense(20, activation=\"relu\"), Dense(1)]\n",
    "model = Sequential(layers)\n",
    "model.compile(\"adam\", \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the neural network on the normalised dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_, y_train_, batch_size=32, epochs=10, validation_data=(X_valid_, y_valid_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "\n",
    "y_pred_test = model.predict(X_test_)\n",
    "model.evaluate(X_test_, y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define (and normalise) inputs bounds\n",
    "\n",
    "MIN_temp_ = (MIN_temp - MEAN_x[0][0]) / STD_x[0][0]\n",
    "MAX_temp_ = (MAX_temp - MEAN_x[0][0]) / STD_x[0][0]\n",
    "MIN_alt_ = (MIN_alt - MEAN_x[0][1]) / STD_x[0][1]\n",
    "MAX_alt_ = (MAX_alt - MEAN_x[0][1]) / STD_x[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d(xt, yt, xtest, ytest, fun, name_figure=\"Braking Distance Estimation\"):\n",
    "\n",
    "    x_temp = np.linspace(MIN_temp_, MAX_temp_, 50)\n",
    "    x_alt = np.linspace(MIN_alt_, MAX_alt_, 50)\n",
    "    res = []\n",
    "    for x0 in x_temp:\n",
    "        for x1 in x_alt:\n",
    "            res.append(fun(np.array([[x0, x1]])))\n",
    "    res = np.array(res)\n",
    "    res = res.reshape((50, 50)).T\n",
    "    X, Y = np.meshgrid(x_temp, x_alt)\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    surf = ax.plot_surface(X, Y, res, cmap=matplotlib.colormaps[\"viridis\"], linewidth=0, antialiased=False, alpha=0.5)\n",
    "\n",
    "    if xt is not None:\n",
    "        ax.scatter(xt[:, 0], xt[:, 1], yt, zdir=\"z\", marker=\".\", c=\"b\", s=100, label=\"Training point (subset)\")\n",
    "    if xtest is not None:\n",
    "        ax.scatter(\n",
    "            xtest[:, 0], xtest[:, 1], ytest, zdir=\"z\", marker=\".\", c=\"k\", s=100, label=\"Validation point (subset)\"\n",
    "        )\n",
    "\n",
    "    ax.set_title(name_figure)\n",
    "    ax.set_xlabel(\"Temperature\")\n",
    "    ax.set_ylabel(\"Altitude\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "plot_3d(X_train_[:1000], y_train_[:1000], X_valid_[:100], y_valid_[:100], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick empirical check on model monotonicity\n",
    "\n",
    "According to the principles of physics, the braking distance should increase with both temperature and altitude i.e., there is a monotonic relationship between the input variables and the braking distance. We check that increasing input values (both for temperature and altitude) results in increasing braking distance for a few set of increasing inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_monotonicity(test):\n",
    "    predicted_braking_distances = model.predict(test)\n",
    "    print(\"Predicted Braking Distances for test inputs:\")\n",
    "    for i, dist in enumerate(predicted_braking_distances):\n",
    "        print(f\"Input {test[i]}: {dist}\")\n",
    "    is_monotonic = np.all(np.diff(predicted_braking_distances) >= 0)\n",
    "    if is_monotonic:\n",
    "        print(\"Monotonicity: True\")\n",
    "    else:\n",
    "        print(\"Monotonicity: False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([[0, 0], [MAX_temp_ / 2, MAX_alt_ / 2], [MAX_temp_, MAX_alt_]])\n",
    "check_monotonicity(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array(\n",
    "    [\n",
    "        [0, MAX_alt_ / 2],\n",
    "        [MAX_temp_ / 5, MAX_alt_ / 2],\n",
    "        [2 * MAX_temp_ / 5, MAX_alt_ / 2],\n",
    "        [3 * MAX_temp_ / 5, MAX_alt_ / 2],\n",
    "        [4 * MAX_temp_ / 5, MAX_alt_ / 2],\n",
    "        [MAX_temp_, MAX_alt_ / 2],\n",
    "    ]\n",
    ")\n",
    "check_monotonicity(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array(\n",
    "    [\n",
    "        [MAX_temp_ / 2, 0],\n",
    "        [MAX_temp_ / 2, MAX_alt_ / 5],\n",
    "        [MAX_temp_ / 2, 2 * MAX_alt_ / 5],\n",
    "        [MAX_temp_ / 2, 3 * MAX_alt_ / 5],\n",
    "        [MAX_temp_ / 2, 4 * MAX_alt_ / 5],\n",
    "        [MAX_temp_ / 2, MAX_alt_],\n",
    "    ]\n",
    ")\n",
    "check_monotonicity(test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP2: Guaranteeing model over estimation using the AIROBAS verification pipeline\n",
    "\n",
    "In a case of braking distance estimation, an under- or over-prediction of a surrogate compared to its original reference model can lead to a very different security risk. See e.g., the following illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"braking_distance_estimation_images/overestimate-braking-distance-estimation.png\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of situations can occur during the landing phase of an aircraft. \n",
    "\n",
    "- If the braking distance is shorter than the remaining runway, the aircraft is in a safe zone. If the surrogate model predicts this braking distance (or smaller), the aircraft would perform a safe landing. If the surrogate model predicts a braking distance that is higher, the aircraft may decide to proceed to an (unnecessary) turn around ... not a safety-critical situation per sÃ© but airlines may not see these unnecessary manoeuvres favorably (e.g., operating cost increase etc.)\n",
    "\n",
    "- If the braking distance is longer than the remaining runway, the aircraft is in a danger zone. If the surrogate predicts this braking distance or higher, the aircraft will receive the necessary warning to turn around. If the surrogate predicts a braking distance that is smaller, the aircraft will receive the information that it is safe to land but will in fact over run the runway ... which of course, presents a hugh safety risk.\n",
    "\n",
    "It is therefore crucial to ensure that not only the surrogate model is highly performant, but also that its predictions are always overestimating the reference model.\n",
    "\n",
    "Let's verify this property.\n",
    "\n",
    "\n",
    "In the rest of the analysis, we will assume that the surrogate model predictions respect monotonicity i.e., that for x1 < x2, f(x1) < f(x2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum and maximum values for inputs (temperature and altitude) perturbed by a given \"noise\" epsilon\n",
    "\n",
    "\n",
    "def compute_input_bounds(\n",
    "    x: np.ndarray, epsilon_tmp: float = 0.1, epsilon_alt: float = 0.1\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute the lower and upper bounds for the input parameters.\n",
    "\n",
    "    Parameters:\n",
    "    x (np.ndarray): Input array containing temperature and altitude values.\n",
    "    epsilon_tmp (float): Perturbation factor for temperature.\n",
    "    epsilon_alt (float): Perturbation factor for altitude.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray]: A tuple containing the lower bound and upper bound arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an epsilon array with perturbation factors for temperature and altitude\n",
    "    epsilon = np.array([epsilon_tmp, epsilon_alt])[None]\n",
    "\n",
    "    # Compute the lower and upper bound by subtracting/adding epsilon from x\n",
    "    lower_bound_input = x - epsilon\n",
    "    upper_bound_input = x + epsilon\n",
    "\n",
    "    return lower_bound_input, upper_bound_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute maximum value of the braking distance from the maximum values of the perturbed inputs\n",
    "# To be compared to the lower bound derived from the surrogate model\n",
    "\n",
    "\n",
    "def compute_cesna_bounds(\n",
    "    x: np.array, epsilon_tmp: float = 0.1, epsilon_alt: float = 0.1, std_y: float = STD_y, mean_y: float = MEAN_y\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the maximum value for the braking distance given input parameters\n",
    "    and their perturbations.\n",
    "\n",
    "    Parameters:\n",
    "    x (np.array): Input array containing temperature and altitude values.\n",
    "    epsilon_tmp (float): Perturbation factor for temperature.\n",
    "    epsilon_alt (float): Perturbation factor for altitude.\n",
    "    std_y (float): Standard deviation used for normalizing the model's output.\n",
    "    mean_y (float): Mean used for normalizing the model's output.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The normalized upper bound braking distances for the perturbed inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the maximum bounds for the input parameters based on perturbations\n",
    "    x_max = compute_input_bounds(x, epsilon_tmp, epsilon_alt)[1]  # return lower and upper bound\n",
    "\n",
    "    # Predict the braking distances for the perturbed inputs using the Cesna landing model\n",
    "    y_min = np.array([cesna_landing(x_max[i, 0], x_max[i, 1]) for i in range(x.shape[0])])\n",
    "\n",
    "    # Normalize the output values: (output - mean_y)/std_y\n",
    "    y_min_norm = (y_min - mean_y) / std_y\n",
    "\n",
    "    # Return the normalized maximum value of braking distance\n",
    "    return y_min_norm[:, None]  # ,np.inf * np.ones_like(y_min) - Note: Uncomment if additional bounds are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"allowed\" bounds for the braking distance prediction i.e., [ymin,inf]\n",
    "\n",
    "\n",
    "def compute_output_bounds(y_min: np.array) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute the output bounds for the braking distance estimation model.\n",
    "\n",
    "    Parameters:\n",
    "    y_min (np.array): The minimum predicted braking distances (lower bound).\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray]: A tuple containing the lower bound (y_min)\n",
    "    and the upper bound (set to infinity for each corresponding element in y_min).\n",
    "    \"\"\"\n",
    "\n",
    "    # The lower bound is given by y_min\n",
    "    lower_bound = y_min\n",
    "\n",
    "    # The upper bound is set to infinity for each element in y_min\n",
    "    upper_bound = np.inf * np.ones_like(y_min)\n",
    "\n",
    "    # Return the lower and upper bounds as a tuple\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the input bound domain parameter\n",
    "input_bound_domain_param = BoundsDomainParameter()\n",
    "\n",
    "# Assign function to compute min and mx values of the input parameters\n",
    "input_bound_domain_param.compute_lb_ub_bounds = compute_input_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize output bound domain parameter\n",
    "output_bound_domain_param = BoundsDomainParameter()\n",
    "\n",
    "# Assign function to compute the lower and upper bounds for the output values\n",
    "output_bound_domain_param.compute_lb_ub_bounds = compute_output_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stability Property - Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property = StabilityProperty(\n",
    "    input_bound_domain_param=input_bound_domain_param, output_bound_domain_param=output_bound_domain_param\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Container --- that encapsulates the problem definition, including a unique identifier (tag_id), the trained model (model), and the stability property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a problem container to encapsulate the problem definition\n",
    "problem = ProblemContainer(\n",
    "    tag_id=1,  # Unique identifier for the problem instance\n",
    "    model=model,  # The trained model to be verified\n",
    "    stability_property=property,  # The stability property to be verified\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification Blocks:\n",
    "\n",
    "- Block 1: CleverhansAdvBlock: used for adversarial attack verification, with specific parameters for targeting, attack direction, and method.\n",
    "- Block 2: DecomonBlock: used for incomplete verification.\n",
    "- Block 3: GMLBrick: used for computing bounds with options for warm start and complete verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [\n",
    "    (CleverhansAdvBlock, {\"index_target\": 0, \"attack_up\": False, \"fgs\": True}),\n",
    "    # CleverhansAdvBlock is used for adversarial attack verification\n",
    "    # index_target=0: only one class is targeted\n",
    "    # attack_up=False: checking for overestimation\n",
    "    # fgs=True: using Fast Gradient Sign method for generating adversarial examples\n",
    "    (DecomonBlock, {}),\n",
    "    # DecomonBlock is used for decompositional verification\n",
    "    (GMLBrick, {\"do_bounds_computation\": True, \"do_warm_start\": True}),\n",
    "    # GMLBrick is used for computing bounds with the options to warm start\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification & Analysis\n",
    "\n",
    "Execute the full_verification_pipeline function with the defined problem, input points, output points, and blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the braking distance of the X_test dataset using the Cesna model\n",
    "output_cesna = compute_cesna_bounds(X_test)  # for cesna model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full verification pipeline with the defined problem, input points, output points, and blocks\n",
    "global_verif = full_verification_pipeline(\n",
    "    problem=problem,\n",
    "    input_points=X_test_,  # normalized inputs\n",
    "    output_points=output_cesna,\n",
    "    blocks_verifier=blocks,\n",
    "    verbose=False,  # Enable verbose output for detailed logging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airobas.verif_pipeline import StatusVerif\n",
    "\n",
    "print(\"Summary of global verification:\")\n",
    "print(np.sum(global_verif.status == StatusVerif.VERIFIED), \"Verified points\")\n",
    "print(np.sum(global_verif.status == StatusVerif.VIOLATED), \"Violated points\")\n",
    "print(np.sum(global_verif.status == StatusVerif.TIMEOUT), \"Timeout points\")\n",
    "print(np.sum(global_verif.status == StatusVerif.UNKNOWN), \"Unknown points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = np.array(global_verif.methods)\n",
    "index_that_concluded = global_verif.index_block_that_concluded\n",
    "methods_concluded = methods[index_that_concluded]\n",
    "\n",
    "# Count the unique values and their counts\n",
    "unique_values, counts = np.unique(methods_concluded, return_counts=True)\n",
    "\n",
    "print(f\"The method(s) {unique_values} concluded on {counts} Points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct model predictions using a shift\n",
    "\n",
    "We see in this first verification phase that the surrogate model will often underestimate the braking distance estimation provided by cesna, thus violating the set safety requirement. In order to minimize or avoid such violation and guarantee safeness, one can apply a (hopefully small) shift to the model prediction. \n",
    "\n",
    "The advantage of such \"shift\" and optimal derivation of its value has been introduced in Ducoffe et al. 2020 (see [here](https://ceur-ws.org/Vol-2560/paper11.pdf)).\n",
    "\n",
    "We will start by applying a small shift that would provide examples of both \"safe\" and \"unsafe\" test points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_model(model: keras.Model, shift: float = 0):\n",
    "    bias = (model.layers[-1].bias).numpy()\n",
    "    return model.layers[-1].bias.assign(bias + shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_model(model=model, shift=0.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_verif = full_verification_pipeline(\n",
    "    problem=problem,\n",
    "    input_points=X_test_,  # normalized inputs\n",
    "    output_points=output_cesna,\n",
    "    blocks_verifier=blocks,\n",
    "    verbose=True,\n",
    ")  # Enable verbose output for detailed logging\n",
    "\n",
    "print(\"Summary of global verification:\")\n",
    "print(np.sum(global_verif.status == StatusVerif.VERIFIED), \"Verified points\")\n",
    "print(np.sum(global_verif.status == StatusVerif.VIOLATED), \"Violated points\")\n",
    "print(np.sum(global_verif.status == StatusVerif.TIMEOUT), \"Timeout points\")\n",
    "print(np.sum(global_verif.status == StatusVerif.UNKNOWN), \"Unknown points\")\n",
    "\n",
    "methods = np.array(global_verif.methods)\n",
    "index_that_concluded = global_verif.index_block_that_concluded\n",
    "methods_concluded = methods[index_that_concluded]\n",
    "unique_values, counts = np.unique(methods_concluded, return_counts=True)\n",
    "\n",
    "print(f\"The method(s) {unique_values} concluded respectively on {counts} Points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the shift makes the model systematically overestimate the braking distance with more points assessed as \"verified\".\n",
    "\n",
    "### Robustness to higher \"noise\"\n",
    "\n",
    "Let's increase the input noise (epsilon 0.1 >> 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cesna = compute_cesna_bounds(X_test, epsilon_tmp=0.4, epsilon_alt=0.4)\n",
    "global_verif = full_verification_pipeline(\n",
    "    problem=problem,\n",
    "    input_points=X_test_,  # normalized inputs\n",
    "    output_points=output_cesna,\n",
    "    blocks_verifier=blocks,\n",
    "    verbose=True,\n",
    ")  # Enable verbose output for detailed logging\n",
    "\n",
    "print(\"Summary of global verification:\")\n",
    "print(np.sum(global_verif.status == StatusVerif.VERIFIED), \"Verified points\")\n",
    "print(np.sum(global_verif.status == StatusVerif.VIOLATED), \"Violated points\")\n",
    "print(np.sum(global_verif.status == StatusVerif.TIMEOUT), \"Timeout points\")\n",
    "print(np.sum(global_verif.status == StatusVerif.UNKNOWN), \"Unknown points\")\n",
    "\n",
    "methods = np.array(global_verif.methods)\n",
    "index_that_concluded = global_verif.index_block_that_concluded\n",
    "methods_concluded = methods[index_that_concluded]\n",
    "unique_values, counts = np.unique(methods_concluded, return_counts=True)\n",
    "\n",
    "print(f\"The method(s) {unique_values} concluded respectively on {counts} Points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a higher input noise worsens the model safety assessment (i.e., increase the number of \"violated\" test points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formal_verif_2D(epsilon_tmp, epsilon_alt, grid_alt=1, grid_temp=1):\n",
    "    n_1D_alt = grid_alt  # number of split along the 'altitude' dimension\n",
    "    n_1D_temp = grid_temp  # number of split along the 'temperature' dimension\n",
    "\n",
    "    # sample for bounding\n",
    "    alt = np.linspace(MIN_alt, MAX_alt, n_1D_alt)\n",
    "    temp = np.linspace(MIN_temp, MAX_temp, n_1D_temp)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    ax.set_xticks(alt)\n",
    "    ax.set_xticklabels(np.floor(alt), rotation=40)\n",
    "    ax.set_yticks(temp)\n",
    "    count_unsafe = 0\n",
    "    error_formal = []\n",
    "\n",
    "    for i, alt_i in enumerate(alt):  # [:-1]):\n",
    "        for j, temp_j in enumerate(temp):  # [:-1]):\n",
    "\n",
    "            # d_x_i = alt[i + 1] - alt[i]\n",
    "            # d_y_j = temp[j + 1] - temp[j]\n",
    "\n",
    "            # x_max_i = alt[i + 1]\n",
    "            # x_max_j = temp[j + 1]\n",
    "            # _, y_max_ij = generate_dataset(1, MIN=[x_max_i, x_max_j], MAX=[x_max_i, x_max_j])\n",
    "\n",
    "            x_tmp = np.array([[temp[j], alt[i]]])\n",
    "            output_cesna = compute_cesna_bounds(x_tmp, epsilon_tmp=epsilon_tmp, epsilon_alt=epsilon_alt)\n",
    "            x_tmp = (x_tmp - MEAN_x) / STD_x\n",
    "\n",
    "            # Run the full verification pipeline with the defined problem, input points, output points, and blocks\n",
    "            global_verif = full_verification_pipeline(\n",
    "                problem=problem,\n",
    "                input_points=x_tmp,\n",
    "                output_points=output_cesna,\n",
    "                blocks_verifier=blocks,\n",
    "                verbose=False,  # Enable verbose output for detailed logging\n",
    "            )\n",
    "\n",
    "            if np.sum(global_verif.status == StatusVerif.VERIFIED) > 0:\n",
    "                color = \"green\"\n",
    "\n",
    "            else:\n",
    "                error_formal.append(i)  # index of non-robust points\n",
    "                count_unsafe = 1\n",
    "                color = \"orange\"\n",
    "\n",
    "            # rect = patches.Rectangle((alt_i, temp_j), d_x_i, d_y_j, color=color)\n",
    "            # ax.add_patch(rect)\n",
    "            ax.scatter([alt_i, alt_i], [temp_j, temp_j], color=color)\n",
    "\n",
    "    if count_unsafe:\n",
    "        plt.title(\n",
    "            \"UNSAFE ðŸ˜² ?: Possible underestimation with epsilon_tmp={:.2f} and epsilon_alt={:.2f}\".format(\n",
    "                epsilon_tmp, epsilon_alt\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        plt.title(\n",
    "            \"SAFE ðŸ˜Š !: no underestimation with epsilon_tmp={:.2f} and epsilon_alt={:.2f}\".format(\n",
    "                epsilon_tmp, epsilon_alt\n",
    "            )\n",
    "        )\n",
    "    # model.layers[-1].bias.assign(bias)\n",
    "\n",
    "    plt.xlim([MIN_alt, MAX_alt])\n",
    "    plt.ylim(MIN_temp, MAX_temp)\n",
    "    plt.grid()\n",
    "    plt.ylabel(\"temperature\")\n",
    "    plt.xlabel(\"pressure altitude\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    formal_verif_2D,\n",
    "    epsilon_tmp=widgets.FloatSlider(value=0.3, min=0, max=1, step=0.1, continuous_update=False),\n",
    "    epsilon_alt=widgets.FloatSlider(value=0.3, min=0, max=1, step=0.1, continuous_update=False),\n",
    "    grid_alt=widgets.IntSlider(value=20, min=1, max=20, step=1, continuous_update=False),\n",
    "    grid_temp=widgets.IntSlider(value=20, min=1, max=20, step=1, continuous_update=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"braking_distance_estimation_images/verification_eps0.3.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here observe a certain localisation in the input space of safety vs. non-safety (green vs. orange) zones.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial, we assess the safety of a braking distance estimation model. \n",
    "\n",
    "We investigate in particular if the surrogate model overestimates (or not) the actual braking distance, even when local noise is added. \n",
    "\n",
    "- We explore the added benefit on shifting model prediction towards safer predictions. \n",
    "- We observe a decrease in model safety with noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
